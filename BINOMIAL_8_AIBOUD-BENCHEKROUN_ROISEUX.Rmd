---
title: 'RRM project: biology project -- methodology report'
author: "Houda Aiboud Benchekroun and Thomas Roiseux (binomial 8)"
date: "`r Sys.Date()`"
output: pdf_document
header-includes: \usepackage{xcolor,colortbl}\usepackage{siunitx}\usepackage{array,float,booktabs,diagbox}\sisetup{group-separator={,},group-minimum-digits={3}}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(results = 'hide')
```
\rule{\linewidth}{0.1pt}
\section{Introduction}
In this paper, we are going to explain how genetics analysis can determine the protein content of different rice species.
Therefore, we are going to present two methods that we will use to explain this target variable.
Further details will be available in the section related to each method.
But before that, for any regression model, it is usually mandatory to perform transformation on the variables.\par
The studied case is a specific variable related to rice, its protein content, chosen among \num{38} other phenotype variables.
We seek to unveil which genes are responsible of the protein level in the rice.
\par
Using this target variable and all the rice genes as explanatory variables, we are going to build a linear model, then the Lasso regression, while firstly omitting the missing values, then imputing them.
\section{Linear model with omission of missing values}
As we have \num{36901} explanatory variables and only \num{413} observations, we must perform some transformations on those variables before constructing a linear model.\par
In this situation, we chose to use the \(K\)-means algorithm to build clusters of variables that have similarities on the rice, to reduce the number of explanatory variables.
We chose \(K=50\) to still have a certain number of variable clusters but, smaller than the original value.\par
On these \num{50} new variables, we apply the linear model, then we go even further by making a stepwise regression:
```{r linmod, echo=FALSE, results='hide'}
source("LM.R")
```
\begin{table}[H]
\centering
\begin{tabular}{@{}|>{}c|S|S|@{}}
\toprule
\rowcolor{cyan}\diagbox{Situation}{Value}&\multicolumn{1}{c|}{\(R^2\)}&\multicolumn{1}{c|}{\(p\)-value}\\
\midrule
\cellcolor{cyan}Before&\cellcolor{red}0.6878&\cellcolor{green}0.006025\\
\midrule
\cellcolor{cyan}After&\cellcolor{red}0.589&\cellcolor{green}1.3e-9\\
\bottomrule
\end{tabular}
\caption{\(p\)-values and \(R^2\) coefficients from the linear model, before and after the stepwise selection}
\label{tab:lm}
\end{table}
According to \hyperref[tab:lm]{Table~\ref*{tab:lm}}, we notice that performing the linear model on such variable is quite correct and the stepwise variable selection further improves the model, as the \(p\)-greatly decreased. We will then confirm that using a cross-validation.
\section{Cross-validation of the linear model}
As we have a small data set, we are going to use cross-validation to determine if the previous linear model is correct.
\section{Lasso regression}