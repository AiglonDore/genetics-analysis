---
title: 'RRM project: biology project -- methodology report'
author: "Houda Aiboud Benchekroun and Thomas Roiseux (binomial 8)"
date: "`r Sys.Date()`"
output: 
  pdf_document: 
    number_sections: yes
header-includes: \usepackage{xcolor,colortbl}\usepackage{siunitx}\usepackage{array,float,booktabs,diagbox}\sisetup{group-separator={,},group-minimum-digits={3}}\definecolor{dgreen}{RGB}{0,180,0}
bibliography: biblio.bib
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(results = 'hide')
```
\rule{\linewidth}{0.1pt}
\section{Introduction}
In this paper, we are going to explain how genetics analysis can determine the protein content of different rice species.
Therefore, we are going to present two methods that we will use to explain this target variable.
Further details will be available in the section related to each method.
But before that, for any regression model, it is usually mandatory to perform transformation on the variables.\par
The studied case is a specific variable related to rice, its protein content, chosen among \num{38} other phenotype variables.
We seek to unveil which genes are responsible of the protein level in the rice.
\par
Using this target variable and all the rice genes as explanatory variables, we are going to build a linear model, then the Lasso regression, while firstly omitting the missing values, then imputing them.
\section{Linear model with omission of missing values}
As we have \num{36901} explanatory variables and only \num{413} observations, we must perform some transformations on those variables before constructing a linear model.\par
In this situation, we chose to use the \(K\)-means algorithm to build clusters of variables that have similarities on the rice, to reduce the number of explanatory variables.
We chose \(K=50\) to still have a certain number of variable clusters but, smaller than the original value.\par
On these \num{50} new variables, we apply the linear model, then we go even further by making a stepwise regression:
\begin{table}[H]
\centering
\begin{tabular}{@{}|>{}c|S|S|@{}}
\toprule
\rowcolor{cyan}\diagbox{Situation}{Value}&\multicolumn{1}{c|}{\(R^2\)}&\multicolumn{1}{c|}{\(p\)-value}\\
\midrule
\cellcolor{cyan}Before&\cellcolor{red}0.71828&\cellcolor{green}3.854e-5\\
\midrule
\cellcolor{cyan}After&\cellcolor{red}0.7067&\cellcolor{green}1.97e-7\\
\bottomrule
\end{tabular}
\caption{\(p\)-values and \(R^2\) coefficients from the linear model, before and after the stepwise selection}
\label{tab:lm}
\end{table}
According to \hyperref[tab:lm]{Table~\ref*{tab:lm}}, we notice that we have a good \(p\)-value and a good \(R^2\) coefficient, even before selecting groups of variables.
This selection allows us to have even better results for the model.
\section{Cross-validation of the linear model}
As we have a small data set, we are going to use cross-validation to determine if the previous linear model is correct. To do so, we are going to use the \(K\)-fold algorithm, with \(K=25\).
Processing this algorithm on our data set reveals that the computed model correctly fits with the data.\par
Using this algorithm, we had the following results:
\begin{table}[H]
\centering
\begin{tabular}{@{}|c|S|@{}}
\toprule
\cellcolor{cyan}\(R^2\)&\cellcolor{dgreen}0.7182\\
\midrule
\cellcolor{cyan}\(p\)-value&\cellcolor{dgreen}3.854e-5\\
\bottomrule
\end{tabular}
\caption{Validation of the previous linear model}
\label{tab:vallm}
\end{table}
The results displayed in \hyperref[tab:vallm]{Table~\ref*{tab:vallm}} proves that the computed linear model fits with our data, even if it could still be improved.
\section{Group-Lasso regression}
After making a linear model with variable selection, we are now going to switch to another method, the Group-Lasso regression.
We still assume that many covariables are correlated together so we will re-use the \(K\)-means algorithm before applying the Group-lasso method, as we already have groups of variables.\par
This regularization method uses both \(\ell_1\) and \(\ell_2\) penalization to reveal the impact of a group of variables on our target variable.\par
This method, as stated in literature[@glasso], can act by two different ways:
\begin{enumerate}
\item The first way, described in , uses overlapping groups. Each variable in a group has a specific weight, depending on the group. This means that a variable can be in several groups at the same time.
\item Another way, where each variable can only appear once in the model, meaning that it can be only in one group. Should this group be removed, then the variable is dropped from the model, among with all other variables from the group.
\end{enumerate}
For the purpose of this project, we choose to use the second way to implement the Group-Lasso model.
\section{Validating the model}
As we have the same number of variable clusters, we will also use a cross-validation with a \(K\)-fold procedure:
\begin{enumerate}
\item A first subset of \qty{80}{\%} of the data, that will be used as a training set.
\item A second one, made of the remaining \qty{20}{\%} data, to test our model.
\end{enumerate}
Using these data set, we will then use a \(K\)-fold procedure to check f our model is viable, regarding the current data.
\section{Comparizon between both models}
Using the previous models, we can now compute several indicators (BIC, AIC,~\ldots). We already have, from the models the \(R^2\) coefficients and the \(p\)-value, that we can also use to know which model fits best with our data. From that, we will be able to extract the variables that predict best our target variable.
\section*{References}
\bibliography{biblio.bib}